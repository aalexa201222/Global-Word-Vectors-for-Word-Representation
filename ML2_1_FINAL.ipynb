{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuydOXTs7w4o"
      },
      "source": [
        "# Vectorial Word Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjOQt-Rgelp"
      },
      "source": [
        "## Background\n",
        "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for machine learning architectures for language processing. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain a de facto point of reference as well as a good starting point. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) (\"the GloVe paper\").\n",
        "\n",
        "Notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment. Some of the tasks will also require addressing the paper directly.\n",
        "\n",
        "---\n",
        "\n",
        "There are 2 types of tasks in this assignment:\n",
        "- coding tasks --- asking you to write code following specifications provided; Most of the tasks come with test cases for sanity-check. Still, if something is not clear, <ins>do ask questions to lab teachers</ins>.\n",
        "- interpretation questions --- asking you to interpret the data or the results of the model\n",
        "\n",
        "Each comes with its predefined points (totaling to 16pt). Some coding tasks have 0 points but solving them will be useful for you.\n",
        "\n",
        "---\n",
        "\n",
        "You are greatly encouraged to add comments to your code describing what particular lines of code do (in general, a great habit to have in your coding life).\n",
        "Additionally please follow these rules when submitting the notebook:\n",
        "\n",
        "* Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
        "* For theoretical questions, put your solution in the `YOUR ANSWER HERE` cell and keep the header(!).\n",
        "* Don't change or delete any initially provided cells, either text or code, unless explicitly instructed to do so.\n",
        "* Don't delete the comment lines `# TEST...` or edit their code cells. The test cells are for sanity checking. Passing them doesn't necessarily means that your code is fine.\n",
        "* Don't change the names of provided functions and variables or arguments of the functions.\n",
        "* Don't clear the output of your code cells.\n",
        "* Don't output unnecessary info (e.g., printing variables for debugging purposes). This clutters the notebook and slows down the grading. You can have print() in the code, but comment them out before submitting the notebook.\n",
        "* Delete those cells that you inserted for your own debuging/testing purposes.\n",
        "* Don't forget to fill in the contribution information.\n",
        "* Test your code and **make sure we can run your notebook** in the colab environment.\n",
        "* A single notebook file (without archiving) per group should be submitted via BB.\n",
        "\n",
        "<font color=\"red\">You following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNT1WNEnlkBC"
      },
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "\n",
        "* Andreas Alexandrou, Sotiris Zenios\n",
        "* All subsections where completed in pair-programming sessions together in order to better understand all the material"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyHEls6Agelp"
      },
      "source": [
        "## Corpus Statistics\n",
        "\n",
        "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics.\n",
        "\n",
        "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a simplistic data structure that counts the number of times each word has appeared within the context of every other word. The definition of a context varies; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word.\n",
        "\n",
        "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
        "\n",
        "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
        "\n",
        "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
        "\n",
        "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
        "\n",
        "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
        "\n",
        "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">he</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcPQmxFagelp"
      },
      "source": [
        "For the purposes of this assignment, we have prepared a co-occurrence matrix over a minimally processed version of the Harry Potter books.\n",
        "\n",
        "(A few interpretation tasks in this assignment presuppose some minimal level of familiarity with the Harry Potter books/films. If no one in your group is familiar with Harry Potter, you might find the [fandom page](https://harrypotter.fandom.com/wiki/Main_Page) useful or the [synopsis sections](https://en.wikipedia.org/wiki/Harry_Potter_and_the_Philosopher%27s_Stone) of the corresponding wiki pages.\n",
        "\n",
        "The pickle file contains three items:\n",
        "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
        "2. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to integers that show the number of co-occurrences between these words.\n",
        "    E.g. `{\"portrait\": {\"harry\": 124, \"said\": 114, ...}, ...}` meaning that the word \"harry\" has appeared in the context of the word \"portrait\" 124 times, etc.\n",
        "3. `X`: a torch LongTensor ${X}$ of size $N \\times N$, where ${X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
        "\n",
        "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4xXGmBsgelp"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch import FloatTensor, LongTensor\n",
        "from typing import Dict, Callable, List\n",
        "# torch.set_printoptions(precision=8) #to increase precision of printing floats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8FXEspuud42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30a2143-22e1-46c3-caa9-71bfda0723dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-01 18:43:05 URL:https://uc5447531e80184c111b5c87a272.dl.dropboxusercontent.com/cd/0/inline/CGto1CovKaxSQzZ2zOK6_G9fpWr0G41dCuCNmwJ_r5TlQM5ux0S8p48BWPG3GIjMGzD8ZikujJoWTPYl53KkImVt-wQdrfC9s_CbaXjP8D0bwpSY2jGE7gbbRPVe8FJG7Xc/file [173580603/173580603] -> \"HP-Counts.p\" [1]\n"
          ]
        }
      ],
      "source": [
        "# this command downloads the pickle file.\n",
        "# This is not a python code, it is a unix code. You can run system commands in jupyter notebooks.\n",
        "!wget -nv -O HP-Counts.p https://www.dropbox.com/scl/fi/dnm7s38j8d0k0bguisiby/HP-Counts.p?rlkey=j0fc11rlnkow7jqb02sel6gz6&dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvW_GgYDgelr",
        "nbgrader": {
          "grade": false,
          "grade_id": "file-open",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "with open(\"HP-Counts.p\", \"rb\") as f:\n",
        "    vocab, contexts, X = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYhg1c9gels"
      },
      "source": [
        "Let's inspect the top 10 most frequent words in the context of the word 'portrait'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Z-eEFcgelt",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b2fb46-ce7a-4ae2-86e0-cb6973e48818"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('harry', 124),\n",
              " ('said', 114),\n",
              " ('hole', 85),\n",
              " ('ron', 57),\n",
              " ('hermione', 54),\n",
              " ('room', 48),\n",
              " ('fat', 45),\n",
              " ('lady', 43),\n",
              " ('common', 37),\n",
              " ('back', 31)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "sorted([(item, value) for item, value in contexts[\"portrait\"].items()], key=lambda x: x[1], reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTRNE9o7gelt"
      },
      "source": [
        "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow the paper in designing an algorithm that can compress the word vectors while retaining most of their informational content.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b>\n",
        "For the resulting vectors to actually be informative, the source corpus should have a size of at least a few billion words; on the contrary, our corpus enumerates merely a million words, so we can't expect our results to be as great.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd45Trplgelu"
      },
      "source": [
        "### From co-occurrence to probabilities\n",
        "\n",
        "Our matrix $X$ is very sparse; most of its elements are zero.\n",
        "\n",
        "Find what the ratio of non-zero elements is.  \n",
        "Check if the matrix is symmetric (think about why it should (not) be).\n",
        "\n",
        "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVb5uBzngelu",
        "nbgrader": {
          "grade": false,
          "grade_id": "non_zero_ratio",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c1.1 [0pt]\n",
        "def non_zero_ratio(sparse_matrix: LongTensor) -> float:\n",
        "    non_zero_count = sum(1 for row in sparse_matrix for value in row if value != 0)\n",
        "    total_items = sum(len(row) for row in sparse_matrix)\n",
        "    return(float(non_zero_count)/float(total_items))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqLY3Kmugelu",
        "nbgrader": {
          "grade": true,
          "grade_id": "non_zero_ratio_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.1\n",
        "assert 0.1 < non_zero_ratio(X) < 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJsqVzO_gelu"
      },
      "source": [
        "We will soon need to perform division and find the logarithm of ${X}$. Neither of the two operations are well-defined for $0$. That's why for further processing we want to have a matrix without any zero elements.\n",
        "\n",
        "Change the matrix's datatype to a `torch.float` and add a small constant to it (e.g. $0.1$) to ensure numerical stability while maintaining sparsity. The obtained matrix will be used in the remaining sections (not the original one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atv-IK8dgelv",
        "nbgrader": {
          "grade": false,
          "grade_id": "X1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c2cc62-26e7-49b0-bd9d-4c9b89532948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.4100e+01, 1.1000e+00, 1.0000e-01,  ..., 1.0000e-01, 5.1000e+00,\n",
            "         1.0000e-01],\n",
            "        [1.1000e+00, 1.0000e-01, 1.0000e-01,  ..., 4.1000e+00, 1.0000e-01,\n",
            "         1.0000e-01],\n",
            "        [1.0000e-01, 1.0000e-01, 1.0000e-01,  ..., 1.1000e+00, 1.0000e-01,\n",
            "         1.0000e-01],\n",
            "        ...,\n",
            "        [1.0000e-01, 4.1000e+00, 1.1000e+00,  ..., 1.3001e+03, 1.0000e-01,\n",
            "         1.0000e-01],\n",
            "        [5.1000e+00, 1.0000e-01, 1.0000e-01,  ..., 1.0000e-01, 5.2410e+02,\n",
            "         6.1000e+00],\n",
            "        [1.0000e-01, 1.0000e-01, 1.0000e-01,  ..., 1.0000e-01, 6.1000e+00,\n",
            "         4.1000e+00]])\n"
          ]
        }
      ],
      "source": [
        "# @title c1.2 [.25pt]\n",
        "X1 = X.to(torch.float)\n",
        "X1 = X1 + 0.1\n",
        "print(X1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OVZ556bgelv",
        "nbgrader": {
          "grade": true,
          "grade_id": "X1_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.2\n",
        "assert non_zero_ratio(X1) == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is5FKXhSgelv"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1ZXji04gelw"
      },
      "source": [
        "From the paper:\n",
        "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appears in the context of word $i$.\n",
        "\n",
        "Complete the function `to_probabilities` that accepts a co-occurrence matrix and returns the probability matrix $P$.\n",
        "\n",
        "_Hint_: Remember broadcasting and `torch.sum()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZQ0Upzogelw",
        "nbgrader": {
          "grade": false,
          "grade_id": "to_probabilities",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c1.3 [.25pt]\n",
        "def to_probabilities(count_matrix: FloatTensor) -> FloatTensor:\n",
        "    row_sums = torch.sum(count_matrix,dim=1,keepdim=True)\n",
        "    return count_matrix/row_sums\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFs3I2hLgelw",
        "nbgrader": {
          "grade": false,
          "grade_id": "to_probabilities_run",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "P = to_probabilities(X1) # note that we use X1 not X here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1ulUiEjgelw",
        "nbgrader": {
          "grade": true,
          "grade_id": "to_probabilities_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c1.3\n",
        "assert P.shape == torch.Size([len(vocab), len(vocab)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrrQT3zegelx"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZwxb3Mgelx"
      },
      "source": [
        "### Probing words\n",
        "\n",
        "From the paper:\n",
        "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
        "\n",
        "Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix ${P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$. If such probability is impossible to compute for input words, return float 0. probability.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBdjawQGgelx",
        "nbgrader": {
          "grade": false,
          "grade_id": "query",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c2.1 [.25pt]\n",
        "def query(word_i: str, word_j: str, vocab: Dict[str, int], prob_matrix: FloatTensor) -> float:\n",
        "    wi = vocab[word_i]\n",
        "    wj = vocab[word_j]\n",
        "    return float(prob_matrix[wi][wj])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSnC-kk-gelx",
        "nbgrader": {
          "grade": true,
          "grade_id": "query_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c2.1\n",
        "assert round(query('harry', 'potter', vocab, P), 5) == 0.00353"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlVGaJVbgelx"
      },
      "source": [
        "Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix ${P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9wW_9WLgelx",
        "nbgrader": {
          "grade": false,
          "grade_id": "probe",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c2.2 [.25pt]\n",
        "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], prob_matrix: FloatTensor) -> float:\n",
        "    return query(word_i,word_k,vocab,prob_matrix)/query(word_j,word_k,vocab,prob_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1_riqA8gely",
        "nbgrader": {
          "grade": true,
          "grade_id": "probe_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c2.2\n",
        "assert round(probe('harry', 'potter', 'stone', vocab, P), 4) == 1.3872"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA4UcvKKgely"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "491Qc7WJgely"
      },
      "source": [
        "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. **Add two pairs of your own word triplets** and experiment on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XVgjB7lgely",
        "nbgrader": {
          "grade": true,
          "grade_id": "probing",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fffcba-02b7-41a0-e84d-8945a18bcae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea wand magic 0.018442942510795767\n",
            "tea wand cup 26.14265487166853\n",
            "\n",
            "harry water portrait 19.556077127097197\n",
            "harry hermione portrait 0.6677205631116542\n"
          ]
        }
      ],
      "source": [
        "print(\"tea\", \"wand\", \"magic\", probe(\"tea\", \"wand\", \"magic\", vocab, P))\n",
        "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
        "print()\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"harry\", \"water\", \"portrait\", probe(\"harry\", \"water\", \"portrait\", vocab, P))\n",
        "print(\"harry\", \"hermione\", \"portrait\", probe(\"harry\", \"hermione\", \"portrait\", vocab, P))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGDY0xucgelz"
      },
      "source": [
        "#### i1 [1pt]\n",
        "Give a brief interpretation of the results you got. Do they correspond to your expectations? Why or why not?\n",
        "\n",
        "*Hint*: When do we expect the ratio value to be high, low or close to 1? Refer to the GloVe paper for guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCC40-wBgelz",
        "nbgrader": {
          "grade": true,
          "grade_id": "interpretation1",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "The correlation between \"harry\" and \"portrait\" with \"water\" and \"potrait\" is high around 20. That shows that \"harry\" is highly assossiated with \"portrait\" but its assossiation with \"water\" is not as high. This is because as we saw in at the begining of the asssignment the word \"portrait\" has the word \"harry\" as the most frequent word, while the word \"water\" has a low appearance ratio. For the other example we can see that both words are either related to \"portrait\", thats why the ratio its close to one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNpZ5cpUgelz"
      },
      "source": [
        "What would happen if we tried probing out-of-domain words? Use the words that the authors report in the paper in the context of \"ice\" and \"steam\" (Table 1). Make your code to clearly print the details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2mZvmINgelz",
        "nbgrader": {
          "grade": true,
          "grade_id": "ice_steam",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fe3c77-a46a-4ebb-ecd3-f75141a2304e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ice steam solid 0.07830312620153405\n",
            "ice steam gas 0.8613343808912856\n",
            "ice steam water 1.27149363325054\n",
            "ice steam fashion 0.8613343808912856\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "print(\"ice\", \"steam\", \"solid\", probe(\"ice\", \"steam\", \"solid\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"gas\", probe(\"ice\", \"steam\", \"gas\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"water\", probe(\"ice\", \"steam\", \"water\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"fashion\", probe(\"ice\", \"steam\", \"fashion\", vocab, P))\n",
        "\n",
        "#P(k|ice)/P(k|steam)    8.9    8.5 × 10−2     1.36    0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydVy5ca4gelz"
      },
      "source": [
        "#### i2 [1pt]\n",
        "Give an interpretation of the results you got. Do they match what the authors report in the paper? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-WHvhh1gel0",
        "nbgrader": {
          "grade": true,
          "grade_id": "interpretation2",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "As we can see in the first example, in our dataset the word \"solid\" has a higher assossiation with the word \"steam\", where as in the paper it is implied that the word \"solid\" has a higher assossiation with the word \"ice\".\n",
        "In the second example, in our dataset the word \"gas\" with the other two words has either the same assossiation with both words or not. In the paper, it is implied that the word \"gas\" has higher assossiation with the word \"steam\" rather than the word \"ice\".\n",
        "The last two examples show similar results.\n",
        "The difference in the first two examples might be caused because of the specific context of the harry potter dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4tjPd5Qgel0"
      },
      "source": [
        "\n",
        "\n",
        "## Dense Vectors\n",
        "\n",
        "Now, we would like to convert these long sparse vectors into short dense ones.\n",
        "\n",
        "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
        "\n",
        "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
        "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
        "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
        "3. > It should be well-defined for all values in $X$.\n",
        "\n",
        "Given these three constraints, each word $i$ in our vocabulary is represented by four vectors:\n",
        "1. A vector $w_i \\in \\mathbb{R}^D$\n",
        "2. A bias $b_i \\in \\mathbb{R}$\n",
        "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
        "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
        "\n",
        "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
        "\n",
        "$F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$.\n",
        "\n",
        "Or equivalently the least squares error $J$ is minimized, where:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$\n",
        "\n",
        "with $f$ being a weighting function, defined as\n",
        "\n",
        "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
        "    (x/x_{max})^\\alpha, & \\text{if $x<x_{max}$}\\\\\n",
        "    1, & \\text{otherwise}.\n",
        "  \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0pPj8cgel0"
      },
      "source": [
        "### Weighting Function\n",
        "\n",
        "Let's start with the last part.\n",
        "\n",
        "Complete the weighting function `weight_fn` which accepts a co-occurrence matrix ${X}$, a maximum value $x_{max}$ and a fractional power $alpha$, and returns the weighted co-occurrence matrix $f({X})$.\n",
        "\n",
        "Then, compute $\\text{X_weighted}$, the matrix ${X}$ after weighting, using the paper's suggested parameters.\n",
        "\n",
        "_Hint_: Note that $f$ is defined pointwise, so our weighting function should also be pointwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NT840aDLgel0",
        "nbgrader": {
          "grade": false,
          "grade_id": "weight_fn",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c3 [.25pt]\n",
        "def weight_fn(X: FloatTensor, x_max: int, alpha: float) -> FloatTensor:\n",
        "    weights = torch.where(X < x_max, (X / x_max) ** alpha, torch.tensor(1.0))\n",
        "    return weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIkY4abRgel0",
        "nbgrader": {
          "grade": false,
          "grade_id": "X_weighted",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "X_weighted = weight_fn(X1, x_max=100, alpha=3/4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuQFRuWrgel0",
        "nbgrader": {
          "grade": true,
          "grade_id": "weight_fn_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c3\n",
        "assert X_weighted.shape == X1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1c6SOjTgel1"
      },
      "source": [
        "Try to get an understanding of how the weighting affects different co-occurrence values (high and low). Think of some word pairs with high and low co-occurrence and look them up in $X$ and in $\\text{X_weighted}$ to get a better idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vImSYqq1gel1",
        "nbgrader": {
          "grade": true,
          "grade_id": "loss_sandbox",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f30509-641f-40b9-d42b-cee83ef0acab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea wand cup 26.14265487166853\n",
            "tea wand cup 11.36323389018632\n",
            "ice steam solid 0.07830312620153405\n",
            "ice steam solid 0.1498288874297237\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "P = to_probabilities(X1)\n",
        "P1 = to_probabilities(X_weighted)\n",
        "\n",
        "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P))\n",
        "print(\"tea\", \"wand\", \"cup\", probe(\"tea\", \"wand\", \"cup\", vocab, P1))\n",
        "print(\"ice\", \"steam\", \"solid\", probe(\"ice\", \"steam\", \"solid\", vocab, P))\n",
        "print(\"ice\", \"steam\", \"solid\", probe(\"ice\", \"steam\", \"solid\", vocab, P1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGc4Dp0agel2"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "The next step is to write the loss function.\n",
        "\n",
        "We can write it as a pointwise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient.\n",
        "\n",
        "Inspecting the formulation of $J$, it is fairly straightforward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
        "\n",
        "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})\\cdot(W\\tilde{W}^T + b + \\tilde{b}^T - log(X))^2$,\n",
        "\n",
        "where $W$, $\\tilde{W}$ are the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ are the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
        "\n",
        "Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f({X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix ${X}$, and computes $J$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA4W3vXjgel2",
        "nbgrader": {
          "grade": false,
          "grade_id": "loss_fn",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c4 [.25pt]\n",
        "def loss_fn(\n",
        "    X_weighted: FloatTensor,\n",
        "    W: FloatTensor,\n",
        "    W_context: FloatTensor,\n",
        "    B: FloatTensor,\n",
        "    B_context: FloatTensor,\n",
        "    X: FloatTensor\n",
        ") -> FloatTensor:\n",
        "   x = ((torch.mm(W, W_context.t()) + B + B_context.t() - torch.log(X))**2) * X_weighted\n",
        "   J = torch.sum(x)\n",
        "   return J\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wti3lircgel2"
      },
      "source": [
        "Let's make sure that we are on a right track. For this we calculate the loss function with toy input: matrices are of size $2 \\times 2$ while bias vectors of size $2 \\times 1$. You can verify the answer manually and with your implementation of `loss_fn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOPO7NLmgel2",
        "nbgrader": {
          "grade": true,
          "grade_id": "loss_fn_tests",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac38deca-098b-41f6-d864-f0c6550aa6e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(45.2391)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# TEST c4\n",
        "toy_X_weighted = torch.FloatTensor([[.5,1],[.2,.1]])\n",
        "toy_X1 = torch.FloatTensor([[2,1],[1,5]])\n",
        "toy_W1 = torch.FloatTensor([[1,2],[1,0]]) # for W\n",
        "toy_W2 = torch.FloatTensor([[0,1],[1,2]]) # for W~\n",
        "toy_b1 = torch.FloatTensor([[0],[2]]) # for b\n",
        "toy_b2 = torch.FloatTensor([[2],[1]]) # for b~\n",
        "\n",
        "loss_fn(toy_X_weighted, toy_W1, toy_W2, toy_b1, toy_b2, toy_X1)\n",
        "# fill the correct value\n",
        "# assert loss_fn(toy_X_weighted, toy_W1, toy_W2, toy_b1, toy_b2, toy_X1).isclose(FloatTensor([???]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DZ1BOV2gel2"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuW6mKUygel2"
      },
      "source": [
        "### GloVe\n",
        "\n",
        "We have the normalized co-occurrence matrix ${X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
        "\n",
        "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors.\n",
        "\n",
        "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding). Each such layer may be viewed as a stand-alone network that can be optimized using the standard procedure we have already seen. It is recommended to read about Embedding class  \n",
        "\n",
        "We will utilize the `nn.Module` class to contain all our embedding layers and streamline their joint optimization.\n",
        "The container class will be responsible for a few things:\n",
        "\n",
        "1. Wrapping the embedding layers:\n",
        "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
        "    2. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
        "    3. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
        "    4. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
        "2. Implementing `forward`, a function that accepts a weighted co-occurrence matrix $f(X)$, the co-occurrence matrix $X$, then finds the embeddings of all words and finally calls `loss_fn` as defined above.\n",
        "3. Implementing `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
        "> ...With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
        "\n",
        "Complete the network class following the above specifications.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpMpVqe-gel2",
        "nbgrader": {
          "grade": true,
          "grade_id": "GloVe",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c5 [1.5pt]\n",
        "class GloVe(torch.nn.Module):\n",
        "    def __init__(self, vocab: Dict[str, int], vector_dim: int=30, device: str=\"cpu\", seed: int=0) -> None:\n",
        "        super(GloVe, self).__init__()\n",
        "        self.device = device\n",
        "        self.vocab_len = len(vocab)\n",
        "        torch.manual_seed(seed)\n",
        "        self.w = torch.nn.Embedding(self.vocab_len, vector_dim).to(device)\n",
        "        self.wc = torch.nn.Embedding(self.vocab_len, vector_dim).to(device)\n",
        "        self.b = torch.nn.Embedding(self.vocab_len, 1).to(device)\n",
        "        self.bc = torch.nn.Embedding(self.vocab_len, 1).to(device)\n",
        "\n",
        "    def forward(self, X_weighted: FloatTensor, X: FloatTensor) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        W = self.w(embedding_input)\n",
        "        return loss_fn(X_weighted, W, self.wc(embedding_input),self.b(embedding_input),self.bc(embedding_input), X)\n",
        "\n",
        "    def get_vectors(self) -> FloatTensor:\n",
        "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
        "        return  self.w(embedding_input) + self.wc(embedding_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWb-G60hXXcw"
      },
      "outputs": [],
      "source": [
        "# TEST c5\n",
        "assert GloVe(vocab, vector_dim=30, seed=0).w.num_embeddings == len(vocab)\n",
        "assert GloVe(vocab, vector_dim=30, seed=0).bc.num_embeddings == len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TFPsnvtgel3"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHNF2C8Jgel3"
      },
      "source": [
        "Training\n",
        "\n",
        "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign).\n",
        "\n",
        "Instantiate the network class you just defined using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs (don't change the default seed value).\n",
        "\n",
        "When writing the training script, remember that your network's forward pass is __already__ computing the loss. Make sure to print a loss value for each epoch.\n",
        "\n",
        "Training won't take too long on a CPU. In case you want to use a GPU, make sure that variables are correctly moved to a GPU with a `device` argument of class `GloVe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAiF4R9ygel3",
        "nbgrader": {
          "grade": true,
          "grade_id": "optim",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c6.1 [1pt]\n",
        "import torch.optim as optim\n",
        "network = GloVe(vocab, vector_dim=30, device=\"cpu\", seed=0)\n",
        "opt = optim.Adam(network.parameters(), lr=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U-QMxyngel3",
        "nbgrader": {
          "grade": true,
          "grade_id": "training",
          "locked": false,
          "points": 0.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96a5a5b-6439-4a4d-96a0-7795e46195da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 11233914.0\n",
            "Epoch 2/300, Loss: 9747190.0\n",
            "Epoch 3/300, Loss: 8462501.0\n",
            "Epoch 4/300, Loss: 7357555.5\n",
            "Epoch 5/300, Loss: 6410067.0\n",
            "Epoch 6/300, Loss: 5599318.5\n",
            "Epoch 7/300, Loss: 4906458.0\n",
            "Epoch 8/300, Loss: 4314543.5\n",
            "Epoch 9/300, Loss: 3808523.5\n",
            "Epoch 10/300, Loss: 3375189.0\n",
            "Epoch 11/300, Loss: 3003075.5\n",
            "Epoch 12/300, Loss: 2682324.0\n",
            "Epoch 13/300, Loss: 2404525.75\n",
            "Epoch 14/300, Loss: 2162565.0\n",
            "Epoch 15/300, Loss: 1950476.625\n",
            "Epoch 16/300, Loss: 1763321.75\n",
            "Epoch 17/300, Loss: 1597080.75\n",
            "Epoch 18/300, Loss: 1448565.125\n",
            "Epoch 19/300, Loss: 1315332.125\n",
            "Epoch 20/300, Loss: 1195598.5\n",
            "Epoch 21/300, Loss: 1088134.625\n",
            "Epoch 22/300, Loss: 992130.125\n",
            "Epoch 23/300, Loss: 907028.25\n",
            "Epoch 24/300, Loss: 832340.375\n",
            "Epoch 25/300, Loss: 767473.4375\n",
            "Epoch 26/300, Loss: 711620.3125\n",
            "Epoch 27/300, Loss: 663749.5\n",
            "Epoch 28/300, Loss: 622689.375\n",
            "Epoch 29/300, Loss: 587262.0\n",
            "Epoch 30/300, Loss: 556409.75\n",
            "Epoch 31/300, Loss: 529274.625\n",
            "Epoch 32/300, Loss: 505218.1875\n",
            "Epoch 33/300, Loss: 483793.375\n",
            "Epoch 34/300, Loss: 464690.8125\n",
            "Epoch 35/300, Loss: 447683.46875\n",
            "Epoch 36/300, Loss: 432583.03125\n",
            "Epoch 37/300, Loss: 419212.9375\n",
            "Epoch 38/300, Loss: 407394.625\n",
            "Epoch 39/300, Loss: 396944.5625\n",
            "Epoch 40/300, Loss: 387678.5625\n",
            "Epoch 41/300, Loss: 379419.53125\n",
            "Epoch 42/300, Loss: 372006.6875\n",
            "Epoch 43/300, Loss: 365303.0625\n",
            "Epoch 44/300, Loss: 359200.03125\n",
            "Epoch 45/300, Loss: 353617.375\n",
            "Epoch 46/300, Loss: 348499.0\n",
            "Epoch 47/300, Loss: 343807.1875\n",
            "Epoch 48/300, Loss: 339515.53125\n",
            "Epoch 49/300, Loss: 335602.5625\n",
            "Epoch 50/300, Loss: 332046.40625\n",
            "Epoch 51/300, Loss: 328822.28125\n",
            "Epoch 52/300, Loss: 325901.0\n",
            "Epoch 53/300, Loss: 323249.96875\n",
            "Epoch 54/300, Loss: 320835.1875\n",
            "Epoch 55/300, Loss: 318623.8125\n",
            "Epoch 56/300, Loss: 316586.25\n",
            "Epoch 57/300, Loss: 314697.46875\n",
            "Epoch 58/300, Loss: 312938.09375\n",
            "Epoch 59/300, Loss: 311293.84375\n",
            "Epoch 60/300, Loss: 309754.59375\n",
            "Epoch 61/300, Loss: 308312.78125\n",
            "Epoch 62/300, Loss: 306962.28125\n",
            "Epoch 63/300, Loss: 305696.8125\n",
            "Epoch 64/300, Loss: 304509.625\n",
            "Epoch 65/300, Loss: 303393.28125\n",
            "Epoch 66/300, Loss: 302339.875\n",
            "Epoch 67/300, Loss: 301341.5625\n",
            "Epoch 68/300, Loss: 300391.0625\n",
            "Epoch 69/300, Loss: 299482.03125\n",
            "Epoch 70/300, Loss: 298608.9375\n",
            "Epoch 71/300, Loss: 297767.4375\n",
            "Epoch 72/300, Loss: 296953.875\n",
            "Epoch 73/300, Loss: 296165.4375\n",
            "Epoch 74/300, Loss: 295399.6875\n",
            "Epoch 75/300, Loss: 294654.8125\n",
            "Epoch 76/300, Loss: 293928.9375\n",
            "Epoch 77/300, Loss: 293220.4375\n",
            "Epoch 78/300, Loss: 292527.71875\n",
            "Epoch 79/300, Loss: 291849.1875\n",
            "Epoch 80/300, Loss: 291183.5\n",
            "Epoch 81/300, Loss: 290529.46875\n",
            "Epoch 82/300, Loss: 289886.0625\n",
            "Epoch 83/300, Loss: 289252.625\n",
            "Epoch 84/300, Loss: 288628.65625\n",
            "Epoch 85/300, Loss: 288013.90625\n",
            "Epoch 86/300, Loss: 287408.28125\n",
            "Epoch 87/300, Loss: 286811.6875\n",
            "Epoch 88/300, Loss: 286224.21875\n",
            "Epoch 89/300, Loss: 285645.84375\n",
            "Epoch 90/300, Loss: 285076.65625\n",
            "Epoch 91/300, Loss: 284516.59375\n",
            "Epoch 92/300, Loss: 283965.6875\n",
            "Epoch 93/300, Loss: 283423.8125\n",
            "Epoch 94/300, Loss: 282890.9375\n",
            "Epoch 95/300, Loss: 282366.9375\n",
            "Epoch 96/300, Loss: 281851.78125\n",
            "Epoch 97/300, Loss: 281345.40625\n",
            "Epoch 98/300, Loss: 280847.75\n",
            "Epoch 99/300, Loss: 280358.71875\n",
            "Epoch 100/300, Loss: 279878.125\n",
            "Epoch 101/300, Loss: 279405.8125\n",
            "Epoch 102/300, Loss: 278941.53125\n",
            "Epoch 103/300, Loss: 278484.9375\n",
            "Epoch 104/300, Loss: 278035.8125\n",
            "Epoch 105/300, Loss: 277593.875\n",
            "Epoch 106/300, Loss: 277158.78125\n",
            "Epoch 107/300, Loss: 276730.34375\n",
            "Epoch 108/300, Loss: 276308.28125\n",
            "Epoch 109/300, Loss: 275892.375\n",
            "Epoch 110/300, Loss: 275482.40625\n",
            "Epoch 111/300, Loss: 275078.1875\n",
            "Epoch 112/300, Loss: 274679.5\n",
            "Epoch 113/300, Loss: 274286.1875\n",
            "Epoch 114/300, Loss: 273898.0625\n",
            "Epoch 115/300, Loss: 273514.96875\n",
            "Epoch 116/300, Loss: 273136.75\n",
            "Epoch 117/300, Loss: 272763.28125\n",
            "Epoch 118/300, Loss: 272394.4375\n",
            "Epoch 119/300, Loss: 272030.21875\n",
            "Epoch 120/300, Loss: 271670.4375\n",
            "Epoch 121/300, Loss: 271315.09375\n",
            "Epoch 122/300, Loss: 270964.15625\n",
            "Epoch 123/300, Loss: 270617.5\n",
            "Epoch 124/300, Loss: 270275.15625\n",
            "Epoch 125/300, Loss: 269937.03125\n",
            "Epoch 126/300, Loss: 269603.09375\n",
            "Epoch 127/300, Loss: 269273.3125\n",
            "Epoch 128/300, Loss: 268947.625\n",
            "Epoch 129/300, Loss: 268626.03125\n",
            "Epoch 130/300, Loss: 268308.5\n",
            "Epoch 131/300, Loss: 267995.0\n",
            "Epoch 132/300, Loss: 267685.53125\n",
            "Epoch 133/300, Loss: 267380.0\n",
            "Epoch 134/300, Loss: 267078.46875\n",
            "Epoch 135/300, Loss: 266780.9375\n",
            "Epoch 136/300, Loss: 266487.25\n",
            "Epoch 137/300, Loss: 266197.375\n",
            "Epoch 138/300, Loss: 265911.34375\n",
            "Epoch 139/300, Loss: 265629.09375\n",
            "Epoch 140/300, Loss: 265350.65625\n",
            "Epoch 141/300, Loss: 265075.84375\n",
            "Epoch 142/300, Loss: 264804.71875\n",
            "Epoch 143/300, Loss: 264537.21875\n",
            "Epoch 144/300, Loss: 264273.3125\n",
            "Epoch 145/300, Loss: 264012.96875\n",
            "Epoch 146/300, Loss: 263756.09375\n",
            "Epoch 147/300, Loss: 263502.65625\n",
            "Epoch 148/300, Loss: 263252.625\n",
            "Epoch 149/300, Loss: 263005.90625\n",
            "Epoch 150/300, Loss: 262762.53125\n",
            "Epoch 151/300, Loss: 262522.40625\n",
            "Epoch 152/300, Loss: 262285.46875\n",
            "Epoch 153/300, Loss: 262051.703125\n",
            "Epoch 154/300, Loss: 261821.046875\n",
            "Epoch 155/300, Loss: 261593.40625\n",
            "Epoch 156/300, Loss: 261368.828125\n",
            "Epoch 157/300, Loss: 261147.234375\n",
            "Epoch 158/300, Loss: 260928.5625\n",
            "Epoch 159/300, Loss: 260712.734375\n",
            "Epoch 160/300, Loss: 260499.78125\n",
            "Epoch 161/300, Loss: 260289.59375\n",
            "Epoch 162/300, Loss: 260082.140625\n",
            "Epoch 163/300, Loss: 259877.40625\n",
            "Epoch 164/300, Loss: 259675.34375\n",
            "Epoch 165/300, Loss: 259475.90625\n",
            "Epoch 166/300, Loss: 259279.03125\n",
            "Epoch 167/300, Loss: 259084.703125\n",
            "Epoch 168/300, Loss: 258892.875\n",
            "Epoch 169/300, Loss: 258703.53125\n",
            "Epoch 170/300, Loss: 258516.59375\n",
            "Epoch 171/300, Loss: 258332.046875\n",
            "Epoch 172/300, Loss: 258149.875\n",
            "Epoch 173/300, Loss: 257970.015625\n",
            "Epoch 174/300, Loss: 257792.421875\n",
            "Epoch 175/300, Loss: 257617.109375\n",
            "Epoch 176/300, Loss: 257444.015625\n",
            "Epoch 177/300, Loss: 257273.09375\n",
            "Epoch 178/300, Loss: 257104.34375\n",
            "Epoch 179/300, Loss: 256937.703125\n",
            "Epoch 180/300, Loss: 256773.1875\n",
            "Epoch 181/300, Loss: 256610.734375\n",
            "Epoch 182/300, Loss: 256450.296875\n",
            "Epoch 183/300, Loss: 256291.890625\n",
            "Epoch 184/300, Loss: 256135.46875\n",
            "Epoch 185/300, Loss: 255980.96875\n",
            "Epoch 186/300, Loss: 255828.4375\n",
            "Epoch 187/300, Loss: 255677.78125\n",
            "Epoch 188/300, Loss: 255528.984375\n",
            "Epoch 189/300, Loss: 255382.0625\n",
            "Epoch 190/300, Loss: 255236.921875\n",
            "Epoch 191/300, Loss: 255093.609375\n",
            "Epoch 192/300, Loss: 254952.046875\n",
            "Epoch 193/300, Loss: 254812.234375\n",
            "Epoch 194/300, Loss: 254674.140625\n",
            "Epoch 195/300, Loss: 254537.71875\n",
            "Epoch 196/300, Loss: 254403.0\n",
            "Epoch 197/300, Loss: 254269.90625\n",
            "Epoch 198/300, Loss: 254138.40625\n",
            "Epoch 199/300, Loss: 254008.515625\n",
            "Epoch 200/300, Loss: 253880.21875\n",
            "Epoch 201/300, Loss: 253753.46875\n",
            "Epoch 202/300, Loss: 253628.203125\n",
            "Epoch 203/300, Loss: 253504.453125\n",
            "Epoch 204/300, Loss: 253382.1875\n",
            "Epoch 205/300, Loss: 253261.375\n",
            "Epoch 206/300, Loss: 253141.96875\n",
            "Epoch 207/300, Loss: 253023.984375\n",
            "Epoch 208/300, Loss: 252907.390625\n",
            "Epoch 209/300, Loss: 252792.15625\n",
            "Epoch 210/300, Loss: 252678.28125\n",
            "Epoch 211/300, Loss: 252565.6875\n",
            "Epoch 212/300, Loss: 252454.40625\n",
            "Epoch 213/300, Loss: 252344.40625\n",
            "Epoch 214/300, Loss: 252235.65625\n",
            "Epoch 215/300, Loss: 252128.125\n",
            "Epoch 216/300, Loss: 252021.84375\n",
            "Epoch 217/300, Loss: 251916.734375\n",
            "Epoch 218/300, Loss: 251812.78125\n",
            "Epoch 219/300, Loss: 251710.03125\n",
            "Epoch 220/300, Loss: 251608.375\n",
            "Epoch 221/300, Loss: 251507.84375\n",
            "Epoch 222/300, Loss: 251408.40625\n",
            "Epoch 223/300, Loss: 251310.03125\n",
            "Epoch 224/300, Loss: 251212.734375\n",
            "Epoch 225/300, Loss: 251116.484375\n",
            "Epoch 226/300, Loss: 251021.265625\n",
            "Epoch 227/300, Loss: 250927.03125\n",
            "Epoch 228/300, Loss: 250833.78125\n",
            "Epoch 229/300, Loss: 250741.53125\n",
            "Epoch 230/300, Loss: 250650.21875\n",
            "Epoch 231/300, Loss: 250559.859375\n",
            "Epoch 232/300, Loss: 250470.4375\n",
            "Epoch 233/300, Loss: 250381.90625\n",
            "Epoch 234/300, Loss: 250294.28125\n",
            "Epoch 235/300, Loss: 250207.515625\n",
            "Epoch 236/300, Loss: 250121.625\n",
            "Epoch 237/300, Loss: 250036.59375\n",
            "Epoch 238/300, Loss: 249952.390625\n",
            "Epoch 239/300, Loss: 249869.0\n",
            "Epoch 240/300, Loss: 249786.4375\n",
            "Epoch 241/300, Loss: 249704.6875\n",
            "Epoch 242/300, Loss: 249623.6875\n",
            "Epoch 243/300, Loss: 249543.46875\n",
            "Epoch 244/300, Loss: 249464.03125\n",
            "Epoch 245/300, Loss: 249385.328125\n",
            "Epoch 246/300, Loss: 249307.34375\n",
            "Epoch 247/300, Loss: 249230.109375\n",
            "Epoch 248/300, Loss: 249153.5625\n",
            "Epoch 249/300, Loss: 249077.71875\n",
            "Epoch 250/300, Loss: 249002.5625\n",
            "Epoch 251/300, Loss: 248928.109375\n",
            "Epoch 252/300, Loss: 248854.328125\n",
            "Epoch 253/300, Loss: 248781.15625\n",
            "Epoch 254/300, Loss: 248708.6875\n",
            "Epoch 255/300, Loss: 248636.84375\n",
            "Epoch 256/300, Loss: 248565.609375\n",
            "Epoch 257/300, Loss: 248495.0\n",
            "Epoch 258/300, Loss: 248425.0\n",
            "Epoch 259/300, Loss: 248355.625\n",
            "Epoch 260/300, Loss: 248286.8125\n",
            "Epoch 261/300, Loss: 248218.609375\n",
            "Epoch 262/300, Loss: 248150.96875\n",
            "Epoch 263/300, Loss: 248083.90625\n",
            "Epoch 264/300, Loss: 248017.359375\n",
            "Epoch 265/300, Loss: 247951.421875\n",
            "Epoch 266/300, Loss: 247886.0\n",
            "Epoch 267/300, Loss: 247821.078125\n",
            "Epoch 268/300, Loss: 247756.75\n",
            "Epoch 269/300, Loss: 247692.90625\n",
            "Epoch 270/300, Loss: 247629.5625\n",
            "Epoch 271/300, Loss: 247566.75\n",
            "Epoch 272/300, Loss: 247504.421875\n",
            "Epoch 273/300, Loss: 247442.578125\n",
            "Epoch 274/300, Loss: 247381.21875\n",
            "Epoch 275/300, Loss: 247320.34375\n",
            "Epoch 276/300, Loss: 247259.9375\n",
            "Epoch 277/300, Loss: 247200.0\n",
            "Epoch 278/300, Loss: 247140.515625\n",
            "Epoch 279/300, Loss: 247081.484375\n",
            "Epoch 280/300, Loss: 247022.90625\n",
            "Epoch 281/300, Loss: 246964.765625\n",
            "Epoch 282/300, Loss: 246907.078125\n",
            "Epoch 283/300, Loss: 246849.78125\n",
            "Epoch 284/300, Loss: 246792.9375\n",
            "Epoch 285/300, Loss: 246736.46875\n",
            "Epoch 286/300, Loss: 246680.46875\n",
            "Epoch 287/300, Loss: 246624.859375\n",
            "Epoch 288/300, Loss: 246569.625\n",
            "Epoch 289/300, Loss: 246514.8125\n",
            "Epoch 290/300, Loss: 246460.359375\n",
            "Epoch 291/300, Loss: 246406.3125\n",
            "Epoch 292/300, Loss: 246352.65625\n",
            "Epoch 293/300, Loss: 246299.359375\n",
            "Epoch 294/300, Loss: 246246.4375\n",
            "Epoch 295/300, Loss: 246193.890625\n",
            "Epoch 296/300, Loss: 246141.703125\n",
            "Epoch 297/300, Loss: 246089.84375\n",
            "Epoch 298/300, Loss: 246038.359375\n",
            "Epoch 299/300, Loss: 245987.21875\n",
            "Epoch 300/300, Loss: 245936.4375\n",
            "CPU times: user 58.4 s, sys: 1min 48s, total: 2min 47s\n",
            "Wall time: 2min 47s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "num_epochs = 300\n",
        "losses = [] # collect losses for each epoch here\n",
        "for i in range(num_epochs):\n",
        "    loss = network.forward(X_weighted, X1) # loss computation (optionally print it out), remember to use X1 counts not X\n",
        "    # opt.zero_grad()\n",
        "    loss.backward()   # gradient computation\n",
        "    opt.step()# back-propagation\n",
        "    opt.zero_grad()  # gradient reset\n",
        "\n",
        "    # Collect losses\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    print(f\"Epoch {i + 1}/{num_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roq7Ubklgel4"
      },
      "source": [
        "Note that if you want to re-run the training process from scratch, remember that you also need to redefine `network` otherwise your training will continue from the last epoch's training state.  \n",
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n6RCKKbgel4",
        "nbgrader": {
          "grade": false,
          "grade_id": "len_losses",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TEST c6.1\n",
        "assert len(losses) == 300\n",
        "assert losses[0] > losses[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3iCGDjDgel4"
      },
      "source": [
        "Plot the losses (x axis for epoch number and y axis for loss) and examine the learning curve. Ask yourself, is its shape what you would expect it to be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emLdJBO_gel4",
        "nbgrader": {
          "grade": true,
          "grade_id": "plot",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "a5b5b0ac-d360-4ea7-a006-def6b253cfe2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title c6.2 [0pt]\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.legend([\"Training\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pM9LESdgel4"
      },
      "source": [
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqu-EFqGgel4"
      },
      "source": [
        "### Validation (Similarity)\n",
        "\n",
        "Curious to see what this network has learned? Let's perform a simple validation experiment.\n",
        "\n",
        "We will check which words the model considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high-dimensional vector spaces is the cosine similarity.\n",
        "\n",
        "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
        "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}|_2 \\cdot |\\vec{b}|_2}$$\n",
        "\n",
        "where $|\\vec{x}|_2$ is the $L_2$-norm of the $\\vec{x}$.\n",
        "\n",
        "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words. For an outside-vocabulary word similarity is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnFdO_3wgel5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ab85deea538994af",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: FloatTensor) -> float:\n",
        "    if not(word_i in vocab and word_i in vocab): return 0.\n",
        "    i = vocab[word_i]\n",
        "    j = vocab[word_j]\n",
        "    v_i = vectors[i] / torch.linalg.vector_norm(vectors[i])  # a/|a|\n",
        "    v_j = vectors[j] / torch.linalg.vector_norm(vectors[j])  # b/|b|\n",
        "    sim = torch.mm(v_i.view(1, -1), v_j.view(-1, 1)).item()\n",
        "    # sim = sim = torch.dot(v_i, v_j)\n",
        "    return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXzC6nzgel5"
      },
      "source": [
        "Let's check out some examples. Consider the word pairs below and, optionally, add your own word pairs if it helps to support your answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ8W10bOgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "similarity_sandbox",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5e2214-3954-4270-d4d9-3d1a85035118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'cruciatus' and 'imperius' is: 0.4987606406211853\n",
            "Similarity between 'avada' and 'kedavra' is: 0.6552069187164307\n",
            "Similarity between 'hogwarts' and 'school' is: 0.7654939293861389\n",
            "Similarity between 'goblin' and 'hagrid' is: 0.27460333704948425\n",
            "Similarity between 'giant' and 'hagrid' is: 0.5027310848236084\n"
          ]
        }
      ],
      "source": [
        "word_vectors = network.get_vectors().detach()\n",
        "\n",
        "for pair in [\n",
        "    (\"cruciatus\", \"imperius\"),\n",
        "    (\"avada\", \"kedavra\"),\n",
        "    (\"hogwarts\", \"school\"),\n",
        "    (\"goblin\", \"hagrid\"),\n",
        "    (\"giant\", \"hagrid\"),\n",
        "]:\n",
        "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' is: {similarity(pair[0], pair[1], vocab, word_vectors)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Oh6nme_gel5"
      },
      "source": [
        "#### i3 [1pt]\n",
        "Give an interpretation of the results. Do the scores correspond well to your perceived similarity of these word pairs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJjjY4PEgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation3",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: 'cruciatus' and 'imperius' (0.499): These are both names of curses in the Harry Potter series, so we would expect them to have contextual similarity. A score of about 0.5 suggests a moderate level of similarity, indicating that while they aren't identical, the model recognizes common context or usage.\n",
        "\n",
        "'avada' and 'kedavra' (0.655): These two words form \"Avada Kedavra,\" another spell from Harry Potter, and are almost always used together, hence the higher similarity score. This shows that the model has effectively learned the strong association between these two words.\n",
        "\n",
        "'hogwarts' and 'school' (0.765): Hogwarts is the primary setting for the Harry Potter series and is a school. The high similarity score indicates that the model has successfully learned this context and understands that 'Hogwarts' is very closely related to the concept of a 'school.'\n",
        "\n",
        "'goblin' and 'hagrid' (0.275): While both terms are from the Harry Potter universe, goblins and Hagrid (a specific character) aren't directly related. Hagrid is a half-giant, not a goblin. The low similarity score reflects the model's recognition that these words are not used in similar contexts.\n",
        "\n",
        "'giant' and 'hagrid' (0.503): Hagrid is a half-giant in the Harry Potter series. The moderate similarity score here suggests that the model somewhat recognizes the relationship between 'Hagrid' and 'giant,' but they don't have a strong association in the text data the model was trained on (perhaps because there are many other contexts in which 'giant' is used beyond discussing Hagrid)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyaWr70ygel5"
      },
      "source": [
        "To obtain the similarities of one word against all other words in the corpus, use torch functions and follow the equation ($\\vec{c}_i$ is $i$th row of $\\mathbf{C}$):\n",
        "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}|_2 \\cdot |\\mathbf{C}|_2} = \\Big(\\frac{\\vec{w}\\cdot \\vec{c}_1}{|\\vec{w}|_2 \\cdot |\\vec{c}_1|_2},\\ldots,\\frac{\\vec{w}\\cdot \\vec{c}_N}{|\\vec{w}|_2 \\cdot |\\vec{c}_N|_2}\\Big)$$\n",
        "\n",
        "Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus. If a word is out of vocabulary, it should return a matrix of 0 similarities.\n",
        "\n",
        "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_9TvkOBgel5",
        "nbgrader": {
          "grade": true,
          "grade_id": "similarities",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c7 [1.5pt]\n",
        "def similarities(word_i: str, vocab: Dict[str, int], vectors: FloatTensor) -> FloatTensor:\n",
        "    # use torch functions. Don't iterate over rows of vectors matrix!\n",
        "    if word_i not in vocab:\n",
        "      return torch.zeros(1, len(vocab))\n",
        "    word_index = vocab[word_i]\n",
        "    word_vector = vectors[word_index]\n",
        "    dot_products = torch.mv(vectors, word_vector)\n",
        "    # Compute the norm of the word vector\n",
        "    word_norm = torch.norm(word_vector)\n",
        "    # Compute the norm of all vectors in the corpus\n",
        "    corpus_norms = torch.norm(vectors, dim=1)\n",
        "    similarities = dot_products / (word_norm * corpus_norms)\n",
        "\n",
        "    return similarities.view(1, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctJObbzAgel5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-74c0c2f09dccce6c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ccd144f-42b1-4f76-91b3-01ab337c5213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4298])\n",
            "torch.Size([1, 4298])\n"
          ]
        }
      ],
      "source": [
        "# TEST c7a\n",
        "print(similarities('harry', vocab, word_vectors).shape)\n",
        "print(torch.Size([1, len(vocab)]))\n",
        "assert similarities('harry', vocab, word_vectors).shape == torch.Size([1, len(vocab)])\n",
        "assert similarities('cow', vocab, word_vectors).shape == torch.Size([1, len(vocab)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM11ebjEgel6"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AaywkIgel6"
      },
      "source": [
        "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word are!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTThuWx1gel6",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b702f18eb2920639",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def most_similar(word_i: str, vocab: Dict[str, int], vectors: FloatTensor, k: int) -> List[str]:\n",
        "    \"\"\" Returns a list of k words that are most similar to word_i\n",
        "        The list excludes word_i itself\n",
        "    \"\"\"\n",
        "    sims = similarities(word_i, vocab, vectors)\n",
        "    _, topi = sims.topk(dim=-1, k=k+1)\n",
        "    topi = topi.view(-1).cpu().numpy().tolist()\n",
        "    inv = {v: i for i, v in vocab.items()}\n",
        "    return [inv[i] for i in topi if inv[i] != word_i][:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uZL5j6qgel6",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-de27a6b9963f32ac",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef747c06-f9c9-40ff-dd14-00d24d761d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'forbidden': ['shadows', 'grounds', 'forest', 'lawns', 'mountains', 'ahead']\n",
            "Most similar words to 'myrtle': ['moaning', 'bathroom', 'hid', 'toilet', 'sobbing', 'properly']\n",
            "Most similar words to 'gryffindor': ['ravenclaw', 'team', 'wood', 'possession', 'quidditch', 'beat']\n",
            "Most similar words to 'wand': ['spoke', 'face', 'shot', 'onto', 'struggled', 'flew']\n",
            "Most similar words to 'quidditch': ['winning', 'saturday', 'team', 'training', 'captain', 'seeker']\n",
            "Most similar words to 'marauder': ['lucky', 'gotten', 'convinced', 'map', 'locked', 'set']\n",
            "Most similar words to 'horcrux': ['sword', 'object', 'goblin', 'destroy', 'horcruxes', 'safe']\n",
            "Most similar words to 'phoenix': ['feather', 'fawkes', 'spinning', 'building', 'connected', 'murder']\n",
            "Most similar words to 'triwizard': ['tournament', 'champions', 'cedric', 'cup', 'champion', 'quidditch']\n",
            "Most similar words to 'screaming': ['frozen', 'scream', 'voices', 'screams', 'cold', 'forced']\n",
            "Most similar words to 'letter': ['letters', 'hedwig', 'written', 'errol', 'ink', 'asking']\n"
          ]
        }
      ],
      "source": [
        "# TEST c7b\n",
        "for word in [\n",
        "    \"forbidden\", \"myrtle\", \"gryffindor\", \"wand\", \"quidditch\", \"marauder\",\n",
        "    \"horcrux\", \"phoenix\", \"triwizard\", \"screaming\", \"letter\"\n",
        "]:\n",
        "    print(f\"Most similar words to '{word}': {most_similar(word, vocab, word_vectors, 6)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXkuHiogel6"
      },
      "source": [
        "#### i4 [1pt]\n",
        "\n",
        "Interpret the results in the context of Harry Potter books.\n",
        "- Do these most similar words make sense (are they actually similar to the query words)?\n",
        "- Are there any patterns you can see in the \"errors\" (the words that you wouldn't consider actually similar to the query word in general, in everyday life)?\n",
        "- Which examples are instances of similarity (if any) and relatedness (if any)?\n",
        "- Any other observations are welcome.\n",
        "\n",
        "Illustrate your answers with examples from your model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBR7DEz_gel7",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation4",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**:  <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        " i)'forbidden': Words like 'grounds', 'forest', 'lawns', 'mountains', and 'ahead' make sense as they are related to the Forbidden Forest in the books.\n",
        "\n",
        "'myrtle': 'Moaning', 'bathroom', 'toilet', 'sobbing' are related to the character Moaning Myrtle who is  associated with bathrooms and crying.\n",
        "\n",
        "'gryffindor': 'Ravenclaw', 'team', 'quidditch', 'beat' are all relevant to the Hogwarts Gryffindor houseand its activities.\n",
        "\n",
        "'wand': This doesn't seem very  accurate, even though 'spoke' and 'shot' could be related to spells with a wand.\n",
        "\n",
        "'triwizard': 'Tournament', 'champions', 'cedric', 'cup', 'champion' are all very highly relevant to the Triwizard Tournament.\n",
        "\n",
        "ii)Some errors can be seen from words that are contextually related in the books but don't have a similar meaning outside of that. E.g.:\n",
        "\n",
        "'wand' and 'flew' are related in the context of magic but not in general everyday life.\n",
        "'phoenix' and 'murder' are related through the story  but are not similar in everyday context.\n",
        "\n",
        "iii)Similarity: 'letter' and 'letters', 'screaming' and 'scream' show lexical similarity. They are similar in their form and their meaning.\n",
        "\n",
        "Relatedness: 'gryffindor' and 'quidditch', 'myrtle' and 'bathroom', 'triwizard' and 'tournament' demonstrate semantic relatedness. They aren't close in meaning but are part of the same context in the Harry Potter books.\n",
        "\n",
        "All in all,the findings suggest that the model effectively identified relevant semantic connections within the Harry Potter texts, with the similarity of certain words corresponding in a logical way to the context of the story.Bus the model does not comprehend the real life correlations between words or understand the Harry Potter universe as humans do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdp1FSNbgel8"
      },
      "source": [
        "Overall it's quite impressive; we managed to encode a meaningful portion of the corpus statistics in only $30$ numbers per word!\n",
        "(A compression ratio of >99%)\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of ${X}$, $x_{max}$, $\\alpha$, the number of epochs and the dimensionality of the vector space).\n",
        "</div>\n",
        "\n",
        "Word vectors, however, contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eypgbwmMgel8"
      },
      "source": [
        "### Validation (Word Analogies)\n",
        "\n",
        "From the paper:\n",
        "> The word analogy task consists of questions like \"$a$ is to $b$ as is $c$ to $?$\" To correctly answer this question, we must find the word $d$ such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
        "\n",
        "Write your own function that performs the word analogy task.\n",
        "\n",
        "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDNEXZL2gel8",
        "nbgrader": {
          "grade": true,
          "grade_id": "analogy",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title c8 [1.5pt]\n",
        "def analogy(\n",
        "    word_a: str, word_b: str, word_c: str, vocab: Dict[str, int], vectors: FloatTensor, k: int\n",
        ") -> List[str]:\n",
        "    \"\"\" Return a list of k words whose vectors are most similar to the solution vector of the analogy.\n",
        "        word_a, word_b, and word_c are never returned as a part of the list\n",
        "    \"\"\"\n",
        "    if any(word not in vocab for word in [word_a, word_b, word_c]):\n",
        "        raise ValueError(\"One or more words are not in the vocabulary.\")\n",
        "\n",
        "    # Get the vectors for words a, b, and c\n",
        "    vector_a = vectors[vocab[word_a]]\n",
        "    vector_b = vectors[vocab[word_b]]\n",
        "    vector_c = vectors[vocab[word_c]]\n",
        "\n",
        "    # Calculate the solution vector using the analogy formula\n",
        "    solution_vector = vector_b - vector_a + vector_c\n",
        "\n",
        "    # Normalize the solution vector\n",
        "    solution_vector = solution_vector / solution_vector.norm()\n",
        "\n",
        "    # Find the word that is closest to the solution vector\n",
        "    similarities = torch.mv(vectors, solution_vector)\n",
        "    closest_word_idx = torch.argmax(similarities).item()\n",
        "    closest_word = next(key for key, value in vocab.items() if value == closest_word_idx)\n",
        "\n",
        "    # Exclude the input words from the results\n",
        "    original_words = {word_a, word_b, word_c}\n",
        "\n",
        "    # Get the most similar words to the closest word\n",
        "    most_similar_words = most_similar(closest_word, vocab, vectors, k + len(original_words))  # Request extra words because we'll remove some\n",
        "\n",
        "    # Remove the original words and the closest word from the similar words list\n",
        "    final_words = [word for word in most_similar_words if word not in original_words and word != closest_word]\n",
        "\n",
        "    return final_words[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxwe5N4Wgel8"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>Show the completed code to your teacher if you have any doubts</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m9PcG8jgel8"
      },
      "source": [
        "Some example triplets to test your analogies on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFkCHKT_gel8",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-3f0e833f60eafb32",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9e9e02-ba6f-4e5c-b686-1da18a9938b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'padma' is to 'parvati' as 'fred' is to ['george', 'okay', 'percy', 'prefect', 'together', 'ron']\n",
            "'avada' is to 'kedavra' as 'expecto' is to ['patronum', 'dementor', 'slowed', 'patronus', 'stag', 'move']\n",
            "'dungeon' is to 'slytherin' as 'tower' is to ['match', 'johnson', 'slytherins', 'hundred', 'seeker', 'beaten']\n",
            "'scabbers' is to 'ron' as 'hedwig' is to ['cage', 'errol', 'owl', 'beak', 'pigwidgeon', 'letter']\n",
            "'ron' is to 'molly' as 'draco' is to ['cornelius', 'minister', 'prime', 'ministry', 'evidence', 'presence']\n",
            "'durmstrang' is to 'viktor' as 'beauxbatons' is to ['krum', 'madame', 'bulgarian', 'tournament', 'captain', 'bagman']\n",
            "'snape' is to 'potions' as 'trelawney' is to ['mcgonagall', 'professor', 'intently', 'binns', 'teacher', 'staff']\n",
            "'harry' is to 'seeker' as 'ron' is to ['score', 'hoops', 'ireland', 'stands', 'sixty', 'quaffle']\n",
            "'magic' is to 'harry' as 'ron' is to ['done', 'could', 'nothing', 'seemed', 'look', 'felt']\n",
            "'hermione' is to 'said' as 'portrait' is to ['password', 'fat', 'cadogan', 'hole', 'common', 'annoyed']\n",
            "'stick' is to 'toilet' as 'draco' is to ['myrtle', 'delighted', 'slide', 'hopeful', 'walked', 'cheerful']\n"
          ]
        }
      ],
      "source": [
        "# TEST c8\n",
        "triplets = [(\"padma\", \"parvati\", \"fred\"),\n",
        "            (\"avada\", \"kedavra\", \"expecto\"),\n",
        "            (\"dungeon\", \"slytherin\", \"tower\"),\n",
        "            (\"scabbers\", \"ron\", \"hedwig\"),\n",
        "            (\"ron\", \"molly\", \"draco\"),\n",
        "            (\"durmstrang\", \"viktor\", \"beauxbatons\"),\n",
        "            (\"snape\", \"potions\", \"trelawney\"),\n",
        "            (\"harry\", \"seeker\", \"ron\"),\n",
        "\n",
        "            (\"magic\", \"harry\", \"ron\"),\n",
        "            (\"hermione\", \"said\", \"portrait\"),\n",
        "            (\"stick\", \"toilet\", \"draco\"),\n",
        "           ]\n",
        "\n",
        "for a, b, c in triplets:\n",
        "    print(\"'{}' is to '{}' as '{}' is to {}\".format(a, b, c, analogy(a, b, c, vocab, word_vectors, 6)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOUQSblcgel9"
      },
      "source": [
        "Some minimal emergent intelligence :) *(hopefully..)*. 🧙‍♀️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_V5ITEpgel9"
      },
      "source": [
        "#### i5 [1pt]\n",
        "\n",
        "Come up with three additional analogies in the context of Harry Potter.\n",
        "Add them to the list of analogies in the code above and run the analogy computation.\n",
        "\n",
        "Interpret the results:\n",
        "- Did the model manage to guess the correct answers to the analogies (taking the first word in the output to be the model's \"guess\")?\n",
        "- Are the correct answers present in the top K words?\n",
        "- Do you see any patterns in the cases when the model didn't solve the task correctly? In other words, when the model's guess was wrong, can you suggest why the model guessed what it guessed?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKvYAag0gel9",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        " For the analogy 'magic' is to 'harry', the expected relationship might be that magic is heavily linked to Harry's identity. The proposed analogy  in relationship for 'Ron' gave words like 'done', 'could', and 'nothing', which are actually the top words when inspecting the top words for 'Ron', which are actions tied to Ron. In the next analogy, 'hermione' is to 'said', the model tries to capture the frequency with which Hermione speaks in the book, yet its analogy with 'portrait' resulted in terms associated with the Gryffindor common room entrance like 'password' and 'fat', but also less directly related terms like 'annoyed'. The last analogy, 'stick' is to 'toilet', isn't a close relationship therefore the output for 'draco' includes 'myrtle' and emotional states like 'delighted' and 'cheerful'. Across these analogies, the model seems to pick up on strong context but also infers relationships based on word occurrences and proximities in the source books. It suggests that while the model can hint at meaningful associations, it's also influenced by the frequency and context of word pairings in the training data, which might not always lead to semantically accurate analogies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ne6B-pqDcW"
      },
      "source": [
        "## Counting co-occurrences\n",
        "\n",
        "In the beginning of the notebook you were provided with a pickle of co-occurrence counts. But how were these counts obtained? You will find this out when you complete this exercise.  \n",
        "\n",
        "First, we obtain the plain text corpus of the Harry Potter book series and place the book files in the current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXyAJQ3Zvffu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9565309-0fb4-4e09-ccd6-32ca89113e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Harry-Potter-Text-Mining'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 109 (delta 0), reused 0 (delta 0), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (109/109), 13.39 MiB | 13.93 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GHUKmaywCYt"
      },
      "outputs": [],
      "source": [
        "!cp Harry-Potter-Text-Mining/Book\\ Text/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QordGEyDXE35"
      },
      "source": [
        "In research, it is relatively common that papers don't include all necessary details for replicating the experiments. This is often due to the lack of space in the paper, overlooking certain details, or simply a bad practice.\n",
        "\n",
        "We will give you several hints on how the co-occurrence matrix was obtained. With the hints and the provided data in the pickle file, you should be able to replicate the exact content of the co-occurrence matrix from the pickle. Here are the hints:\n",
        "\n",
        "1.   Tokenization of the data was done in a shallow but effective way: replace all punctuations with white space and delimit tokens with a sequence of white spaces;\n",
        "2.   Standard libraries were used for identifying punctuations and stopwords;\n",
        "3.   Context windows are symmetric and don't span across chapters (i.e., the last word of chapter $N$ is not co-occuring with the first word of chapter $N+1$).\n",
        "4.   The vocabulary was obtained based on the frequency cutoff.\n",
        "\n",
        "To replicate the co-occurrence counts, complete the function below. Then use it to find out the exact values of `cutoff` and `window_size` that result in the co-occurrence counts identical to the provided ones. No need to provide the code for finding the exact values. Just use them in the next TEST code cell.\n",
        "\n",
        "Feel free to import any standard python library. Use the same cell code for importing libraries and defining auxiliary functions (if any).\n",
        "The code will be evaluated not only on the correctness of the output but also on efficiency. It should be able to extract the counts and create the tensor in less than a minute (in the colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwE8faHMwqwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4991615e-7d94-4260-d00e-8383fd01ee9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# @title c9 [3pt]\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Dict, Tuple\n",
        "import torch\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "def read_book_files(files: List[str], cutoff: int=10, window_size: int=20, verbose: bool=False) -> Tuple[Dict[str, int], Dict[str, Dict[str, int]], torch.Tensor]:\n",
        "\n",
        "    # Create a translation table that keeps \"1\" but replaces other digits and punctuation with spaces\n",
        "    exclude_digits = string.digits.replace(\"1\", \"\")\n",
        "    translator = str.maketrans(string.punctuation + exclude_digits, ' ' * (len(string.punctuation) + len(exclude_digits)))\n",
        "\n",
        "\n",
        "    word_freq = Counter()\n",
        "\n",
        "    # First pass: Build frequency distribution\n",
        "    for file in files:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            text = f.read().lower()\n",
        "            text = text.translate(translator)\n",
        "            text = re.sub(r\"\\b11+\\b\", \" \", text)  # This regex ensures that sequences of \"11+\" that appear as standalone words are replaced\n",
        "            words = text.split()\n",
        "            word_freq.update(words)\n",
        "\n",
        "    # Filter and sort the vocabulary\n",
        "    vocab_candidates = {word for word, freq in word_freq.items() if freq >= cutoff and word not in stop_words }\n",
        "    sorted_vocab = sorted(vocab_candidates)\n",
        "    voc = {word: idx for idx, word in enumerate(sorted_vocab)}\n",
        "\n",
        "    dd_cnt = defaultdict(lambda: defaultdict(int))\n",
        "    total_words = len(voc)\n",
        "    X = torch.zeros((total_words, total_words), dtype=torch.int64)\n",
        "\n",
        "    # Second pass: Build co-occurrence matrix and dd_cnt\n",
        "    for file in files:\n",
        "      with open(file, 'r', encoding='utf-8') as f:\n",
        "          text = f.read().lower()\n",
        "          text = text.replace('\"text\"@\"chapter\"@\"book\"', '')\n",
        "          chapters = re.split(r'@\\d+@\\d+', text)\n",
        "\n",
        "          for chapter in chapters:\n",
        "              translator = str.maketrans(string.punctuation, ' ' * (len(string.punctuation)))\n",
        "\n",
        "              chapter = chapter.translate(translator)\n",
        "              words = [word for word in chapter.split() ]\n",
        "\n",
        "              for i, word in enumerate(words):\n",
        "                  word_idx = voc.get(word)\n",
        "                  if word_idx is not None:\n",
        "                      start = max(0, i - window_size)\n",
        "                      end = min(len(words), i + window_size)\n",
        "                      co_words = words[start:i] + words[i+1:end+1]\n",
        "                      co_word_counts = Counter(co_words)\n",
        "                      for co_word, count in co_word_counts.items():\n",
        "                          co_word_idx = voc.get(co_word)\n",
        "                          if co_word_idx is not None:\n",
        "                              dd_cnt[word][co_word] += count\n",
        "                              X[word_idx, co_word_idx] += count\n",
        "\n",
        "    return voc, dd_cnt, X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWtcGMkLpvY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749a0a22-3107-47ff-e2c8-1c5a7dc5a344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 9s, sys: 525 ms, total: 2min 10s\n",
            "Wall time: 2min 10s\n"
          ]
        }
      ],
      "source": [
        "# TEST c9\n",
        "# YOUR CODE HERE - Fill in the found cutoff and window_size values\n",
        "%%time\n",
        "my_vocab, my_contexts, my_X = read_book_files([ f\"HPBook{n}.txt\" for n in range(1,8) ],cutoff=16,window_size=25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST COMPARING THE CONTEXT AND X ARRAYS\n",
        "def compare_dictionaries(dict1, dict2):\n",
        "\n",
        "    for key in dict1:\n",
        "        if dict1[key] != dict2[key]:\n",
        "            print(key)\n",
        "            print(dict1[key])\n",
        "            print(dict2[key])\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def compare_matrices(matrix1, matrix2):\n",
        "    if matrix1.shape != matrix2.shape:\n",
        "        return False\n",
        "\n",
        "    return torch.all(torch.eq(matrix1, matrix2)).item()  # .item() is used to get the value from the tensor\n",
        "\n",
        "\n",
        "\n",
        "# Now you can compare them\n",
        "are_dicts_equal = compare_dictionaries(my_contexts, contexts)\n",
        "are_matrices_equal = compare_matrices(my_X, X)\n",
        "\n",
        "print(f\"Dictionaries are equal: {are_dicts_equal}\")\n",
        "print(f\"Matrices are equal: {are_matrices_equal}\")"
      ],
      "metadata": {
        "id": "1atrorX06cv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1782db-2596-4076-b923-bb5a962b3f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionaries are equal: True\n",
            "Matrices are equal: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST TO CHECK IF BOTH VOCABS ARE THE SAME\n",
        "\n",
        "# Generate sets from the keys of the dictionaries\n",
        "vocab_keys = set(my_vocab.keys())\n",
        "context_keys = set(vocab.keys())  # assuming my_dd_cnt is your 'my_context'\n",
        "\n",
        "# Find words that are in context_keys but not in vocab_keys\n",
        "in_context_not_vocab = context_keys - vocab_keys\n",
        "\n",
        "# Find words that are in vocab_keys but not in context_keys\n",
        "in_vocab_not_context = vocab_keys - context_keys\n",
        "\n",
        "# Output the results\n",
        "print(f\"Words in vocab_keys but not in my_vocab_keys: {in_context_not_vocab}\")\n",
        "print(f\"Words in my_vocab_keys but not in vocab_keys: {in_vocab_not_context}\")"
      ],
      "metadata": {
        "id": "qrAKZziYXwsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8af2a38-0dd4-4c95-e5b9-a411414ae0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in vocab_keys but not in my_vocab_keys: set()\n",
            "Words in my_vocab_keys but not in vocab_keys: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t29uPQsQhBnr"
      },
      "source": [
        "<font color=\"red\">**Don't clear the output of the above cell!**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwjIMegPnE7_"
      },
      "source": [
        "#### i6 [1pt]\n",
        "\n",
        "What are the values of `cutoff` and `window_size` that replicates the provided counts?\n",
        "\n",
        "Briefly describe the process of finding the correct underlying algorithm of `read_book_files` and the correct values of `cutoff` and `window_size`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIslxPDHnBWh",
        "nbgrader": {
          "grade": true,
          "grade_id": "Interpretation5",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "source": [
        "**ANSWER**: <font color=\"red\">YOUR ANSWER HERE</font>\n",
        "\n",
        "cutoff=16\n",
        "\n",
        "window size=25\n",
        "\n",
        "The goal was to create a co-occurrence matrix from the Harry Potter books, considering specific criteria such as a certain frequency threshold, excluding all stopwords, and only incorporating the number \"1\" and not other digits. To achieve this, initial efforts focussed on text cleaning: keeping the digit \"1\", discarding other numbers, and stripping punctuation. This process was further explored to manage sequences where numbers like  \"11\" appeared. Once the text was prepared, a frequency distribution (word_freq) was created by iterating through each book, using a Counter to keep track. Using this distribution, a  vocabulary (voc) was formed, considering the frequency threshold and removing defined stopwords.\n",
        "\n",
        "Then moving on to the building the co-occurrence matrix. A sliding window was used: for each word, its adjacent words (within the set window) were pinpointed, and their co-occurrence frequency adjusted. It was essential to view chapters as separate units, preventing cross-chapter word co-occurrence. The solution underwent several refinements, addressing issues like enhanced translation logic, handling chapter separations, and excluding headers such as \"Text\"@\"Chapter\"@\"Book\".\n",
        "\n",
        "The process concluded with experimenting with the best parameters for the task. Through testing , we found that exact values that replicate the original files were cutoff  16 and the window size  25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h2l-Sw7gel9"
      },
      "source": [
        "### Optional\n",
        "If you are done, you can continue experimenting in order to understand the system's behaviour better. For example: how does training and hyperparameter choice affect the model's performance?\n",
        "Repeat the training using your own hyperparameters (vector space dimensionality, optimizer parameters, the number of training epochs, a different random seed, etc.).\n",
        "\n",
        "During the training loop, print the qualitative benchmarks every few epochs. Do they keep improving? Is there any disadvantage to exhaustively training until convergence?\n",
        "\n",
        "Now you have all the tools at hand to train the word vectors on any textual corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbgNN_S8W9Y6"
      },
      "source": [
        "# Acknowledgment\n",
        "\n",
        "The jupyter notebook was initially created by Konstantinos Kogkalidis and Tejaswini Deoskar.  \n",
        "Recent changes, including adaptation to the Colab environment and the exercise on reconstructing a co-occurrence matrix, are by Lasha Abzianidze."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-showmarkdowntxt": false,
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}